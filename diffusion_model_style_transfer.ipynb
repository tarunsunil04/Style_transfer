{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bed0022-d95e-4905-b9ac-762410dc80c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tarun Sunil\\.conda\\envs\\dmt_new\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTextModel, CLIPTokenizer\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec055f15-a288-4df6-9547-b7bbd2e0dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533676e9-e2bb-4f9b-9e10-d5a9922bcf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66d1445-29f7-44e9-923c-7d88c1e48c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "style_prompt = \"in the style of a renaissance painting\"\n",
    "num_steps = 50\n",
    "guidance_scale = 7.5\n",
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "948f4ce5-8fc4-4b1b-a32c-2be6be176b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\").to(device)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "scheduler = DDIMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff84216-dcee-42aa-8374-92a48f3016c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load(\"ViT-L/14\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77238fb0-bb96-4f2a-b392-5ce6323a070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def div_by_8(x):\n",
    "#     return x - (x % 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1db6a9e-adca-436b-8176-bc2e7ae44e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_tokens = clip.tokenize([style_prompt]).to(device)\n",
    "with torch.no_grad():\n",
    "    style_embedding = clip_model.encode_text(clip_tokens).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6880dd28-cb61-4698-8e0c-fa2981a79a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Image.open(\"objects.jpg\").convert(\"RGB\").resize((512, 512))\n",
    "# o_width, o_height = input_image.size\n",
    "# a_width, a_height = div_by_8(o_width), div_by_8(o_height)\n",
    "# resized_input = input_image.resize((a_width, a_height), resample=Image.LANCZOS)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7ecdb-bd9a-4930-81e4-da6da77ac9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a5969db-9aac-460c-81a0-2d7ecde08480",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token = clip.tokenize([style_prompt]).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = clip_model.encode_text(text_token).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5b0fbbe-cbe9-4e98-af10-ae7fd03ba32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = preprocess(input_image).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    init_latents = vae.encode(image_tensor * 2 - 1).latent_dist.sample() * 0.18215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f98f9da-c924-4821-b6b4-af0cf9cbba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer(style_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81ff13e9-4e3e-4512-b217-39e05675302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = init_latents.clone().detach().requires_grad_(True)\n",
    "optimizer = torch.optim.Adam([latents], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d22d127b-3c5e-4f8a-ba18-223dc57a9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "# with autocast(\"cuda\"):\n",
    "#     image = pipe(prompt).images[0]  \n",
    "    \n",
    "# image.save(\"astronaut_rides_horse.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aae5f9fb-dc74-407d-93a6-5b0ab4b0cbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Optimization:   2%|██▍                                                                                                                        | 1/50 [00:28<23:23, 28.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 - CLIP Loss: -0.1476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Optimization:  22%|██████████████████████████▊                                                                                               | 11/50 [06:59<25:30, 39.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 - CLIP Loss: -0.2035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Optimization:  42%|███████████████████████████████████████████████████▏                                                                      | 21/50 [13:30<19:03, 39.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20 - CLIP Loss: -0.2142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Optimization:  62%|███████████████████████████████████████████████████████████████████████████▋                                              | 31/50 [20:04<12:33, 39.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30 - CLIP Loss: -0.2227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Optimization:  82%|████████████████████████████████████████████████████████████████████████████████████████████████████                      | 41/50 [26:39<05:55, 39.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40 - CLIP Loss: -0.2312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Optimization: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [32:33<00:00, 39.07s/it]\n"
     ]
    }
   ],
   "source": [
    "clip_model.eval()\n",
    "vae.eval()\n",
    "\n",
    "for step in trange(num_steps, desc=\"CLIP Optimization\"):\n",
    "    noise_pred = unet(latents, torch.tensor([scheduler.timesteps[0]], device=device), encoder_hidden_states=text_embeddings).sample\n",
    "    latents_denoised = latents - noise_pred  # simplified step\n",
    "        \n",
    "    decoded = vae.decode(latents_denoised / 0.18215).sample\n",
    "    decoded = (decoded.clamp(-1, 1) + 1) / 2\n",
    "    clip_input = torch.nn.functional.interpolate(decoded, size=224, mode=\"bicubic\", align_corners=False)\n",
    "    clip_input = (clip_input - 0.5) / 0.5\n",
    "    image_features = clip_model.encode_image(clip_input).float()\n",
    "\n",
    "    loss = -torch.cosine_similarity(image_features, style_embedding).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step} - CLIP Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "686c6893-a052-4d97-9b01-7cc5b7d69b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    final_image = vae.decode(latents / 0.18215).sample\n",
    "    final_image = (final_image.clamp(-1, 1) + 1) / 2\n",
    "    final_image = final_image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    final_image = (final_image * 255).astype(np.uint8)\n",
    "    # final_image = final_image.resize((orig_w, orig_h), Image.LANCZOS)\n",
    "    Image.fromarray(final_image).save(\"clip_guided_stylized_output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4adca5f-c2d2-4580-8700-2ffaa9e667a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.clear_autocast_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4222756-11d0-4b44-adad-57e5c4f76522",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0215c264-b427-402f-8870-148ec9d56083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
